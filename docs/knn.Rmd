---
title: 'K Nearest Neighbours (KNN)'
output:
  html_document:
    number_sections: yes
    toc: yes
    toc_float: yes
  pdf_document:
    number_sections: yes
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduccion a la clasificacion

## Lectura de datos

```{r}
d = read.csv("datos/MichelinNY.csv")
str(d)
```


# Algoritmo K-vecinos más próximos

- Datos: (yi, x1i, x2i,...,xpi), i = 1,...,n

| y   | x1  | x2  |     | xp  |
|:---:|:---:|:---:|:---:|:---:|
| y1  | x11 | x21 | ... | xp1 | 
| y1  | x12 | x22 | ... | xp2 |
| ... | ... | ... | ... | ... | 
| y1  | x1n | x2n | ... | xpn | 


- Se calcula la distancia euclídea del dato que se quiere clasificar (x1a, x2a,...,xpa) con cada uno de los puntos de la base de datos

$$
d_i = \sqrt{(x_{1i} - x_{1a})^2 + (x_{2i} - x_{2a})^2 + \cdots + (x_{pi} - x_{pa})^2 }
$$

- se ordenan las distancias de menor a mayor y se le asigna al nuevo dato la categoría mayoritaria dentro de los k-datos con menor distancia.

- es decir, si K = 1, se le asigna la categoría del punto más cercano.

- se suelen utilizar K impares para evitar empates.

-  a menudo se utilizan regresores estandarizados para que todos los regresores tengan la misma contribución a la distancia.

# KNN para clasificación

Los regresores cualitativos son dificiles de modelar (cual es la distancia entre colores, por ejemplo?). Por eso no los vamos a incluir en el análisis (punto débil del algoritmo).

## Training, validation and test sets

- Seleccionamos 80% de los datos para el training set y 20% de los datos para el test set:

```{r}
set.seed(123)
n = nrow(d)
pos_train = sample(1:n,round(0.8*n), replace = F)
train_x = d[pos_train,3:6]
test_x = d[-pos_train,3:6]
train_y = d$InMichelin[pos_train]
test_y = d$InMichelin[-pos_train]
```

## Resultado

- k = 1

```{r}
library(class)
test_pred = knn(train_x, test_x, train_y, k = 1)
table(test_y,test_pred)
```

- k = 3

```{r}
test_pred = knn(train_x, test_x, train_y, k = 3)
table(test_y,test_pred)
```

## Escalado de variables

Para que todas las variables tengan la misma importancia, escalamos las variables numericas:

```{r}
d$Food1 = (d$Food - min(d$Food))/(max(d$Food) - min(d$Food))
plot(d$Food1)
```

```{r}
normalization = function(x){
  x1 = (x - min(x))/(max(x) - min(x))
  return(x1)
}
```

```{r}
d$Decor1 = normalization(d$Decor)
d$Service1 = normalization(d$Service)
d$Price1 = normalization(d$Price)
```

```{r}
train_x = d[pos_train,7:10]
test_x = d[-pos_train,7:10]
test_pred = knn(train_x, test_x, train_y, k = 1)
table(test_y,test_pred)
```

Como vemos, la clasificación mejora.

## Comparación con regresión logística

```{r}
train = d[pos_train,]
test = d[-pos_train,]
m = glm(InMichelin ~ Food + Decor + Service + Price, data = train, family = binomial)
prob = predict(m, newdata = test, type = "response")
```

```{r}
pred = rep(0,length(prob))
pred[prob > 0.5] = 1
# Matriz de confusion
table(test$InMichelin, pred)
```

Como se observa, el modelo logit predice mejor.

## Analisis de los datos IRIS

```{r}
d = iris
str(d)
```

```{r}
d$Sepal.Length1 = normalization(d$Sepal.Length)
d$Sepal.Width1 = normalization(d$Sepal.Width)
d$Petal.Length1 = normalization(d$Sepal.Length)
d$Petal.Width1 = normalization(d$Petal.Width)
```

```{r}
set.seed(123)
n = nrow(d)
pos_train = sample(1:n,round(0.8*n), replace = F)
train_x = d[pos_train,6:9]
test_x = d[-pos_train,6:9]
train_y = d[pos_train,5]
test_y = d[-pos_train,5]
```


```{r}
pred = knn(train_x,test_x,train_y, k=3)
```

```{r}
table(test_y, pred)
```

Calculamos el valor optimo de k:

```{r}
kv = c(1,3,5,7)
pred = rep(0,4)
ii = 1
for (i in kv){
  pred_i = knn(train_x,test_x,train_y, k=i)
  pred[ii] = sum(diag(table(test_y,pred_i)))
  ii = ii + 1
}
plot(kv,pred)
```

# KNN para regresion

```{r}
library(FNN)
```


```{r}
d = read.csv("datos/kidiq.csv")
#d$mom_hs = factor(d$mom_hs, labels = c("no", "si"))
#d$mom_work = factor(d$mom_work, labels = c("notrabaja", "trabaja23", "trabaja1_parcial", "trabaja1_completo"))
```


```{r}
set.seed(123)
n = nrow(d)
pos_train = sample(1:n,round(0.8*n), replace = F)
train_x = d[pos_train,c(3,5)]
test_x = d[-pos_train,c(3,5)]
train_y = d[pos_train,1]
test_y = d[-pos_train,1]
```

```{r}
p = knn.reg(train_x, test_x, train_y, k = 1)
(mse = mean(test_y-p$pred)^2 )
```

## Incluir variables cualitativas

```{r}
d$mom_hs1 = ifelse(d$mom_hs == 1,1,0)
d$mom_hs0 = ifelse(d$mom_hs == 0,1,0)
```

```{r}
train_x = d[pos_train,c(3,5,6,7)]
test_x = d[-pos_train,c(3,5,6,7)]
train_y = d[pos_train,1]
test_y = d[-pos_train,1]
```

```{r}
p = knn.reg(train_x, test_x, train_y, k = 1)
(mse = mean(test_y-p$pred)^2 )
```

## Incluir dos regresores cualitativos

```{r}
d$mom_work1 = ifelse(d$mom_work == 1,1,0)
d$mom_work2 = ifelse(d$mom_work == 2,1,0)
d$mom_work3 = ifelse(d$mom_work == 3,1,0)
d$mom_work4 = ifelse(d$mom_work == 4,1,0)
```

```{r}
train_x = d[pos_train,c(3,5:11)]
test_x = d[-pos_train,c(3,5:11)]
train_y = d[pos_train,1]
test_y = d[-pos_train,1]
```

```{r}
p = knn.reg(train_x, test_x, train_y, k = 1)
(mse = mean(test_y-p$pred)^2 )
```